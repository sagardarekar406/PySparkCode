from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lower, when, count, avg, current_date, trim
)


spark = SparkSession.builder \
    .appName("UserDataProcessingPipeline") \
    .getOrCreate()


users_df = spark.read.csv(
    "/data/input/users.csv",
    header=True,
    inferSchema=True
)

users_df = users_df.dropDuplicates(["user_id"])


users_df = users_df.filter(
    (col("age").isNotNull()) & (col("age") > 0)
)


users_df = users_df.withColumn("email", trim(col("email"))) \
                   .withColumn("country", trim(col("country")))



users_df = users_df.withColumn(
    "email",
    lower(col("email"))
)


users_df = users_df.withColumn(
    "is_adult",
    when(col("age") >= 18, True).otherwise(False)
)


users_df = users_df.withColumn(
    "processed_date",
    current_date()
)


country_summary_df = users_df.groupBy("country").agg(
    count("*").alias("user_count"),
    avg("age").alias("average_age")
)


country_summary_df.write.mode("overwrite").parquet(
    "/data/output/country_user_summary"
)

spark.stop()
